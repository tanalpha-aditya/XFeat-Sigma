{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103457a5",
   "metadata": {
    "papermill": {
     "duration": 0.003497,
     "end_time": "2023-04-16T16:15:57.085492",
     "exception": false,
     "start_time": "2023-04-16T16:15:57.081995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SiLK Submission\n",
    "\n",
    "This is the official IMC2022 results of the [SiLK](https://github.com/facebookresearch/silk) keypoint model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fa711f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-16T16:15:57.093149Z",
     "iopub.status.busy": "2023-04-16T16:15:57.092612Z",
     "iopub.status.idle": "2023-04-16T16:15:57.103573Z",
     "shell.execute_reply": "2023-04-16T16:15:57.102682Z"
    },
    "papermill": {
     "duration": 0.017577,
     "end_time": "2023-04-16T16:15:57.105764",
     "exception": false,
     "start_time": "2023-04-16T16:15:57.088187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dry_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b1c75b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T16:15:57.112005Z",
     "iopub.status.busy": "2023-04-16T16:15:57.111649Z",
     "iopub.status.idle": "2023-04-16T16:17:52.629713Z",
     "shell.execute_reply": "2023-04-16T16:17:52.628432Z"
    },
    "papermill": {
     "duration": 115.524247,
     "end_time": "2023-04-16T16:17:52.632417",
     "exception": false,
     "start_time": "2023-04-16T16:15:57.108170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchtext 0.14.0\r\n",
      "Uninstalling torchtext-0.14.0:\r\n",
      "  Successfully uninstalled torchtext-0.14.0\r\n",
      "Found existing installation: torchaudio 0.13.0\r\n",
      "Uninstalling torchaudio-0.13.0:\r\n",
      "  Successfully uninstalled torchaudio-0.13.0\r\n",
      "Found existing installation: fastai 2.7.12\r\n",
      "Uninstalling fastai-2.7.12:\r\n",
      "  Successfully uninstalled fastai-2.7.12\r\n",
      "\u001b[33mWARNING: Skipping allennlp as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mLooking in links: /kaggle/input/imc2022-dependencies-silk/wheels\r\n",
      "Processing /kaggle/input/imc2022-dependencies-silk/wheels/torch-1.11.0+cu113-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11.0+cu113) (4.4.0)\r\n",
      "Installing collected packages: torch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.13.0\r\n",
      "    Uninstalling torch-1.13.0:\r\n",
      "      Successfully uninstalled torch-1.13.0\r\n",
      "Successfully installed torch-1.11.0+cu113\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mLooking in links: /kaggle/input/imc2022-dependencies-silk/wheels\r\n",
      "Processing /kaggle/input/imc2022-dependencies-silk/wheels/torchvision-0.12.0+cu113-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchvision==0.12.0+cu113) (4.4.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.12.0+cu113) (2.28.2)\r\n",
      "Requirement already satisfied: torch==1.11.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.12.0+cu113) (1.11.0+cu113)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.12.0+cu113) (1.21.6)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.12.0+cu113) (9.4.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0+cu113) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0+cu113) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0+cu113) (2022.12.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0+cu113) (1.26.14)\r\n",
      "Installing collected packages: torchvision\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.14.0\r\n",
      "    Uninstalling torchvision-0.14.0:\r\n",
      "      Successfully uninstalled torchvision-0.14.0\r\n",
      "Successfully installed torchvision-0.12.0+cu113\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mLooking in links: /kaggle/input/imc2022-dependencies-silk/wheels\r\n",
      "Processing /kaggle/input/imc2022-dependencies-silk/wheels/hydra_core-1.3.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2022-dependencies-silk/wheels/omegaconf-2.3.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2022-dependencies-silk/wheels/antlr4_python3_runtime-4.9.3-py3-none-any.whl\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from hydra-core) (23.0)\r\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from hydra-core) (5.10.2)\r\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.7/site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources->hydra-core) (3.11.0)\r\n",
      "Installing collected packages: antlr4-python3-runtime, omegaconf, hydra-core\r\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.1 omegaconf-2.3.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mLooking in links: /kaggle/input/imc2022-dependencies-silk/wheels\r\n",
      "Processing /kaggle/input/imc2022-dependencies-silk/wheels/loguru-0.6.0-py3-none-any.whl\r\n",
      "Installing collected packages: loguru\r\n",
      "Successfully installed loguru-0.6.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torchtext torchaudio fastai allennlp\n",
    "\n",
    "# offline\n",
    "!pip install -f /kaggle/input/imc2022-dependencies-silk/wheels --no-index torch==1.11.0+cu113\n",
    "!pip install -f /kaggle/input/imc2022-dependencies-silk/wheels --no-index torchvision==0.12.0+cu113\n",
    "!pip install -f /kaggle/input/imc2022-dependencies-silk/wheels --no-index hydra-core\n",
    "!pip install -f /kaggle/input/imc2022-dependencies-silk/wheels --no-index loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927c0405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T16:17:52.643233Z",
     "iopub.status.busy": "2023-04-16T16:17:52.642211Z",
     "iopub.status.idle": "2023-04-16T16:17:59.005212Z",
     "shell.execute_reply": "2023-04-16T16:17:59.004004Z"
    },
    "papermill": {
     "duration": 6.371084,
     "end_time": "2023-04-16T16:17:59.007799",
     "exception": false,
     "start_time": "2023-04-16T16:17:52.636715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import csv\n",
    "import cv2\n",
    "from functools import partial\n",
    "import random\n",
    "from torchvision.transforms.functional import resize, InterpolationMode\n",
    "\n",
    "sys.path.append(os.path.join('/kaggle/input/silk-kp/silk-main/silk-main'))\n",
    "sys.path.append(os.path.join('/kaggle/input/silk-kp/silk-main/silk-main/scripts/examples'))\n",
    "\n",
    "import silk\n",
    "import common\n",
    "from silk.backbones.silk.silk import from_feature_coords_to_image_coords\n",
    "\n",
    "conf = {\n",
    "    \"common\": {\n",
    "        \"paths\": {\n",
    "            \"images\": \"/kaggle/input/image-matching-challenge-2022/test_images/\",\n",
    "        },\n",
    "        \"nms\": 0, # 0 = disabled\n",
    "        \"device\": torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        \"topk\": 30_000,\n",
    "        \"ransac\": {\n",
    "            \"max_iter\": 200_000,\n",
    "            \"confidence\": 0.99999,\n",
    "            \"reproj_threshold\": 0.25,\n",
    "        }\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"checkpoint\": \"/kaggle/input/silk-kp/coco-rgb-aug.ckpt\",\n",
    "        \"image_max_side\": 720,\n",
    "        \"matcher\": {\n",
    "            \"postprocessing\": \"double-softmax\",\n",
    "            \"threshold\": 0.99,\n",
    "            \"temperature\": 0.1,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "common.DEVICE = conf[\"common\"][\"device\"] # hacky\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d1454a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T16:17:59.018121Z",
     "iopub.status.busy": "2023-04-16T16:17:59.017818Z",
     "iopub.status.idle": "2023-04-16T16:17:59.229300Z",
     "shell.execute_reply": "2023-04-16T16:17:59.228326Z"
    },
    "papermill": {
     "duration": 0.21941,
     "end_time": "2023-04-16T16:17:59.231511",
     "exception": false,
     "start_time": "2023-04-16T16:17:59.012101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FlattenMatrix(M, num_digits=8):\n",
    "    '''Convenience function to write CSV files.'''\n",
    "    \n",
    "    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n",
    "\n",
    "\n",
    "def BuildCompositeImage(im1, im2, axis=1, margin=0, background=1):\n",
    "    '''Convenience function to stack two images with different sizes.'''\n",
    "    \n",
    "    if background != 0 and background != 1:\n",
    "        background = 1\n",
    "    if axis != 0 and axis != 1:\n",
    "        raise RuntimeError('Axis must be 0 (vertical) or 1 (horizontal')\n",
    "\n",
    "    h1, w1, _ = im1.shape\n",
    "    h2, w2, _ = im2.shape\n",
    "\n",
    "    if axis == 1:\n",
    "        composite = np.zeros((max(h1, h2), w1 + w2 + margin, 3), dtype=np.uint8) + 255 * background\n",
    "        if h1 > h2:\n",
    "            voff1, voff2 = 0, (h1 - h2) // 2\n",
    "        else:\n",
    "            voff1, voff2 = (h2 - h1) // 2, 0\n",
    "        hoff1, hoff2 = 0, w1 + margin\n",
    "    else:\n",
    "        composite = np.zeros((h1 + h2 + margin, max(w1, w2), 3), dtype=np.uint8) + 255 * background\n",
    "        if w1 > w2:\n",
    "            hoff1, hoff2 = 0, (w1 - w2) // 2\n",
    "        else:\n",
    "            hoff1, hoff2 = (w2 - w1) // 2, 0\n",
    "        voff1, voff2 = 0, h1 + margin\n",
    "    composite[voff1:voff1 + h1, hoff1:hoff1 + w1, :] = im1\n",
    "    composite[voff2:voff2 + h2, hoff2:hoff2 + w2, :] = im2\n",
    "\n",
    "    return (composite, (voff1, voff2), (hoff1, hoff2))\n",
    "\n",
    "\n",
    "def DrawMatches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n",
    "    '''Draw keypoints and matches.'''\n",
    "    \n",
    "    composite, v_offset, h_offset = BuildCompositeImage(im1, im2, axis, margin, background)\n",
    "\n",
    "    # Draw all keypoints.\n",
    "    for coord_a, coord_b in zip(kp1, kp2):\n",
    "        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n",
    "        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n",
    "    \n",
    "    # Draw matches, and highlight keypoints used in matches.\n",
    "    for idx_a, idx_b in matches:\n",
    "        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n",
    "        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n",
    "#         color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "        color = (0, 0, 255)\n",
    "        composite = cv2.line(composite,\n",
    "                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n",
    "                                   int(kp1[idx_a][1] + v_offset[0])]),\n",
    "                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n",
    "                                   int(kp2[idx_b][1] + v_offset[1])]), color=color, thickness=1)\n",
    "    \n",
    "    return composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14eec6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T16:17:59.241233Z",
     "iopub.status.busy": "2023-04-16T16:17:59.240943Z",
     "iopub.status.idle": "2023-04-16T16:17:59.251184Z",
     "shell.execute_reply": "2023-04-16T16:17:59.250303Z"
    },
    "papermill": {
     "duration": 0.017629,
     "end_time": "2023-04-16T16:17:59.253412",
     "exception": false,
     "start_time": "2023-04-16T16:17:59.235783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the pairs file.\n",
    "src = '/kaggle/input/image-matching-challenge-2022/'\n",
    "\n",
    "test_samples = []\n",
    "with open(f'{src}/test.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for i, row in enumerate(reader):\n",
    "        # Skip header.\n",
    "        if i == 0:\n",
    "            continue\n",
    "        test_samples += [row]\n",
    "\n",
    "if dry_run:\n",
    "    for sample in test_samples:\n",
    "        print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9d1ea70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T16:17:59.262652Z",
     "iopub.status.busy": "2023-04-16T16:17:59.262379Z",
     "iopub.status.idle": "2023-04-16T16:17:59.290192Z",
     "shell.execute_reply": "2023-04-16T16:17:59.289173Z"
    },
    "papermill": {
     "duration": 0.034751,
     "end_time": "2023-04-16T16:17:59.292139",
     "exception": false,
     "start_time": "2023-04-16T16:17:59.257388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_silk(model, image, max_size = 1024):\n",
    "    assert image.shape[0] == 1\n",
    "    \n",
    "    # get scaling factor\n",
    "    if max_size is not None:\n",
    "        scale = max(max(image.shape[-2:]) / max_size, 1.)\n",
    "    else:\n",
    "        scale = 1.\n",
    "\n",
    "    # resize image if necessary\n",
    "    if scale != 1.:\n",
    "        image = resize(image, size = (int(image.shape[2] / scale), int(image.shape[3] / scale)), interpolation=InterpolationMode.BILINEAR)\n",
    "\n",
    "    # to greyscale if necessary\n",
    "    if image.shape[1] == 3:\n",
    "        image = torchvision.transforms.functional.rgb_to_grayscale(image)\n",
    "\n",
    "    keypoints, descriptors, prob = model(image)\n",
    "    keypoints = from_feature_coords_to_image_coords(model, keypoints)\n",
    "    descriptors = descriptors.reshape(1, 128, -1).permute(0, 2, 1)\n",
    "\n",
    "    keypoints = keypoints[0] * scale\n",
    "    descriptors = descriptors[0]\n",
    "    prob = prob[0]\n",
    "    \n",
    "    return keypoints, descriptors, prob\n",
    "\n",
    "def get_top_k(keypoints, descriptors, k):\n",
    "    positions, scores = keypoints[:,:2], keypoints[:,2]\n",
    "\n",
    "    # top-k selection\n",
    "    idxs = scores.argsort()[-k:]\n",
    "\n",
    "    return positions[idxs], descriptors[idxs] / 1.41, scores[idxs]\n",
    "\n",
    "def extract(model, max_side, top_k, *images):   \n",
    "    all_positions = []\n",
    "    all_descriptors = []\n",
    "    for image in images:\n",
    "        positions, descriptors, _ = run_silk(model, image, max_size = max_side)\n",
    "        positions, descriptors, scores = get_top_k(positions, descriptors, k = top_k)\n",
    "        \n",
    "        positions = positions[:,[1,0]]\n",
    "        descriptors = descriptors * 1.41\n",
    "        \n",
    "        all_positions.append(positions)\n",
    "        all_descriptors.append(descriptors)\n",
    "        \n",
    "    return all_positions, all_descriptors\n",
    "\n",
    "def inlier_matches(inlier_mask, matches):\n",
    "    if inlier_mask is not None:\n",
    "        matches_after_ransac = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\n",
    "    else:\n",
    "        matches_after_ransac = np.array([])\n",
    "        \n",
    "    return matches_after_ransac\n",
    "    \n",
    "\n",
    "def display(batch_id, image_1_id, image_2_id, positions_1, positions_2, matches, *crops):\n",
    "    src = conf[\"common\"][\"paths\"][\"images\"]\n",
    "    image_1 = cv2.cvtColor(cv2.imread(f'{src}/{batch_id}/{image_1_id}.png'), cv2.COLOR_BGR2RGB)\n",
    "    image_2 = cv2.cvtColor(cv2.imread(f'{src}/{batch_id}/{image_2_id}.png'), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    im_inliers = DrawMatches(image_1, image_2, positions_1, positions_2, matches)\n",
    "    \n",
    "    for crop in crops:\n",
    "        x, y, dx, dy = crop\n",
    "        color = (0, 255, 0)\n",
    "        im_inliers = cv2.line(im_inliers,tuple([int(x), int(y)]),tuple([int(x+dx), int(y)]),color=color,thickness=1)\n",
    "        im_inliers = cv2.line(im_inliers,tuple([int(x), int(y)]),tuple([int(x), int(y+dy)]),color=color,thickness=1)\n",
    "        im_inliers = cv2.line(im_inliers,tuple([int(x+dx), int(y)]),tuple([int(x+dx), int(y+dy)]),color=color,thickness=1)\n",
    "        im_inliers = cv2.line(im_inliers,tuple([int(x), int(y+dy)]),tuple([int(x+dx), int(y+dy)]),color=color,thickness=1)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    plt.title(f'{image_1_id}-{image_2_id}')\n",
    "    plt.imshow(im_inliers)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def write_submission(F_dict):\n",
    "    with open('submission.csv', 'w') as f:\n",
    "        f.write('sample_id,fundamental_matrix\\n')\n",
    "        for sample_id, F in F_dict.items():\n",
    "            f.write(f'{sample_id},{FlattenMatrix(F)}\\n')\n",
    "\n",
    "    if dry_run:\n",
    "        !cat submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58f1c73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T16:17:59.301400Z",
     "iopub.status.busy": "2023-04-16T16:17:59.301098Z",
     "iopub.status.idle": "2023-04-16T16:18:06.382388Z",
     "shell.execute_reply": "2023-04-16T16:18:06.381172Z"
    },
    "papermill": {
     "duration": 7.088293,
     "end_time": "2023-04-16T16:18:06.384592",
     "exception": false,
     "start_time": "2023-04-16T16:17:59.296299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/cloud_io.py:42: LightningDeprecationWarning: `pytorch_lightning.utilities.cloud_io.load` has been deprecated in v1.8.0 and will be removed in v2.0.0. This function is internal but you can copy over its implementation.\n",
      "  \"`pytorch_lightning.utilities.cloud_io.load` has been deprecated in v1.8.0 and will be\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process googleurban;1cf87530;a5a9975574c94ff9a285f58c39b53d2c-0143f47ee9e54243a1b8454f3e91621a\n",
      "n matches:3301\n",
      "n inliers:720\n",
      "process googleurban;6ceaefff;39563e58b2b7411da3f06427c9ee4239-0303b05ca0cb46959eac430e4b2472ca\n",
      "n matches:3559\n",
      "n inliers:637\n",
      "process googleurban;d91db836;81dd07fb7b9a4e01996cee637f91ca1a-0006b1337a0347f49b4e651c035dfa0e\n",
      "n matches:1673\n",
      "n inliers:61\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = common.get_model(\n",
    "    checkpoint=conf[\"model\"][\"checkpoint\"],\n",
    "    nms=conf[\"common\"][\"nms\"],\n",
    "    device=conf[\"common\"][\"device\"],\n",
    ")\n",
    "\n",
    "# create matcher\n",
    "matcher = silk.models.silk.matcher(\n",
    "    postprocessing=conf[\"model\"][\"matcher\"][\"postprocessing\"],\n",
    "    threshold=conf[\"model\"][\"matcher\"][\"threshold\"],\n",
    "    temperature=conf[\"model\"][\"matcher\"][\"temperature\"],\n",
    ")\n",
    "\n",
    "F_dict = {}\n",
    "for i, row in enumerate(test_samples):\n",
    "    sample_id, batch_id, image_1_id, image_2_id = row\n",
    "    print(f\"process {sample_id}\")\n",
    "\n",
    "    batch_path = os.path.join(conf[\"common\"][\"paths\"][\"images\"], batch_id)\n",
    "    image_1_path = os.path.join(batch_path, f\"{image_1_id}.png\")\n",
    "    image_2_path = os.path.join(batch_path, f\"{image_2_id}.png\")\n",
    "\n",
    "    # load\n",
    "    images_1 = common.load_images(image_1_path)\n",
    "    images_2 = common.load_images(image_2_path)\n",
    "\n",
    "    # extract\n",
    "    (positions_1, positions_2), (descriptors_1, descriptors_2) = extract(\n",
    "        model,\n",
    "        conf[\"model\"][\"image_max_side\"],\n",
    "        conf[\"common\"][\"topk\"],\n",
    "        images_1,\n",
    "        images_2,\n",
    "    )\n",
    "\n",
    "    # match\n",
    "    matches = matcher(descriptors_1, descriptors_2)\n",
    "    matches = matches.cpu().numpy()\n",
    "\n",
    "    # estimate fundamental matrix\n",
    "    cur_F, inlier_mask = cv2.findFundamentalMat(\n",
    "        positions_1.cpu().numpy()[matches[:, 0]],\n",
    "        positions_2.cpu().numpy()[matches[:, 1]],\n",
    "        cv2.USAC_MAGSAC,\n",
    "        ransacReprojThreshold=conf[\"common\"][\"ransac\"][\"reproj_threshold\"],\n",
    "        confidence=conf[\"common\"][\"ransac\"][\"confidence\"],\n",
    "        maxIters=conf[\"common\"][\"ransac\"][\"max_iter\"],\n",
    "    )\n",
    "\n",
    "    print(f\"n matches:{matches.shape[0]}\")\n",
    "    print(f\"n inliers:{inlier_mask.sum()}\")\n",
    "\n",
    "    F_dict[sample_id] = cur_F\n",
    "\n",
    "    # optional display\n",
    "    if dry_run:\n",
    "        display(batch_id, image_1_id, image_2_id, positions_1.cpu().numpy(), positions_2.cpu().numpy(), inlier_matches(inlier_mask, matches))\n",
    "\n",
    "write_submission(F_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 140.862934,
   "end_time": "2023-04-16T16:18:08.665553",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-16T16:15:47.802619",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
